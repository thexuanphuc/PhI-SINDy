{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "import numpy as np\n",
    "import torch as T\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch_optimizer as optim_all\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_true_model(x, t, omega=1., zeta=0.05, F0=1., forcing_freq=1.2, friction_force_ratio=0.5):\n",
    "    \"\"\"\n",
    "    A function that gets the displacement, velocity and time as an input, and returns the true vector field output (velocity and acceleration)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray \n",
    "\n",
    "    t : ndarray\n",
    "    \n",
    "    omega : float\n",
    "    \n",
    "    zeta : float\n",
    "    \n",
    "    F0 : float\n",
    "\n",
    "    forcing_freq : float\n",
    "\n",
    "    friction_force_ratio : float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        an array with the two vector field values, for the given input\n",
    "    \"\"\"\n",
    "    return [\n",
    "        x[1],\n",
    "        - 2 * omega * zeta * x[1]\n",
    "        - omega ** 2 * x[0]\n",
    "        - friction_force_ratio * np.sign(x[1])\n",
    "        + F0 * np.cos(forcing_freq * t)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficients dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoeffsDictionary(T.nn.Module):\n",
    "    \"\"\"\n",
    "    A class for initializing, storing, and updating the ksi coefficients\n",
    "    These coefficients are the linear weights of a neural network\n",
    "    The class inherits from the torch.nn.Module\n",
    "    \"\"\"\n",
    "    def __init__(self, n_combinations):\n",
    "\n",
    "        super(CoeffsDictionary, self).__init__()\n",
    "        self.linear = T.nn.Linear(n_combinations, 1, bias=False)\n",
    "        # Setting the weights to zeros\n",
    "        self.linear.weight = T.nn.Parameter(0 * self.linear.weight.clone().detach())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_features(x, t, poly_order=2, phases=(), sgn_flag=False, torch_flag=True):\n",
    "    \"\"\"\n",
    "    Applies the feature candidates to the given data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "\n",
    "    t : torch.Tensor\n",
    "    \n",
    "    poly_order : int\n",
    "    \n",
    "    phases : iterable\n",
    "    \n",
    "    sgn_flag : bool\n",
    "    \n",
    "    torch_flag : bool\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray or torch.Tensor\n",
    "    \n",
    "    \"\"\"\n",
    "    ret = T.column_stack(\n",
    "        (\n",
    "            *[T.cos(ph * t) for ph in phases], # trigonometric features\n",
    "            *[T.sign(x[:, k]) for k in (0, 1)], # signum features\n",
    "            *[T.ones(size=(x.shape[0],)), x[:, 0], x[:, 1],  \n",
    "                        x[:, 0] ** 2, x[:, 0] * x[:, 1], x[:, 1] ** 2, \n",
    "                        x[:, 0] ** 3, x[:, 0] ** 2 * x[:, 1], x[:, 0] * x[:, 1] ** 2, x[:, 1] ** 3,]# polynomial features\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if torch_flag:\n",
    "        return ret\n",
    "    else:\n",
    "        return np.array(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4th order Runge-Kutta constraint function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rk4_SparseId(x, LibsCoeffs, times, timesteps, Params, paramSize, phases):\n",
    "    \"\"\"\n",
    "    A function that applies the fourth order Runge-Kutta scheme to the given data in order to derive the ones in the following timestep\n",
    "    During this process the approximate derivatives are used\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    d1 = apply_features(x, times, phases=phases)\n",
    "    k1 = T.column_stack((x[:, 1].unsqueeze(1), (- 2 * Params.zeta * Params.omega * x[:, 1] - Params.omega ** 2 * x[:, 0]).unsqueeze(1) + T.cos(Params.forcing_freq * times) + LibsCoeffs(d1)))\n",
    "    \n",
    "    xtemp = x + 0.5 * timesteps * k1\n",
    "    d2 = apply_features(xtemp, times + 0.5 * timesteps, phases=phases)\n",
    "    k2 = T.column_stack((xtemp[:, 1].unsqueeze(1), (- 2 * Params.zeta * Params.omega * xtemp[:, 1] - Params.omega ** 2 * xtemp[:, 0]).unsqueeze(1) + T.cos(Params.forcing_freq * (times + 0.5 * timesteps)) + LibsCoeffs(d2)))\n",
    "\n",
    "    xtemp = x + 0.5 * timesteps * k2\n",
    "    d3 = apply_features(xtemp, times + 0.5 * timesteps, phases=phases)\n",
    "    k3 = T.column_stack((xtemp[:, 1].unsqueeze(1), (- 2 * Params.zeta * Params.omega * xtemp[:, 1] - Params.omega ** 2 * xtemp[:, 0]).unsqueeze(1) + T.cos(Params.forcing_freq * (times + 0.5 * timesteps)) + LibsCoeffs(d3)))\n",
    "\n",
    "    xtemp = x + timesteps * k3\n",
    "    d4 = apply_features(xtemp, times + timesteps, phases=phases)\n",
    "    k4 = T.column_stack((xtemp[:, 1].unsqueeze(1), (- 2 * Params.zeta * Params.omega * xtemp[:, 1] - Params.omega ** 2 * xtemp[:, 0]).unsqueeze(1) + T.cos(Params.forcing_freq * (times + 0.5 * timesteps)) + LibsCoeffs(d4)))\n",
    "\n",
    "    return x + (1 / 6) * (k1 + 2 * k2 + 2 * k3 + k4) * timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_sparse_model(coeffs, train_set, times, params, param_size, phases=(1.1, 1.2, 1.3), lr_reduction=10):\n",
    "    \"\"\"\"\n",
    "    A function that calculates which ksi coefficients lead to optimal prediction\n",
    "    The updating of the coefficients is performed in a deep learning fashion\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coeffs : CoeffsDictionary object\n",
    "\n",
    "    train_set : torch.Tensor\n",
    "\n",
    "    times : torch.Tensor\n",
    "\n",
    "    params : parameters dataclass\n",
    "\n",
    "    param_size : int\n",
    "\n",
    "    phases : tuple\n",
    "\n",
    "    lr_reduction : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    coeffs\n",
    "\n",
    "    loss_track\n",
    "    \"\"\"\n",
    "    # Define optimizer\n",
    "    opt_func = optim_all.RAdam(\n",
    "        coeffs.parameters(), lr=params.lr, weight_decay=params.weightdecay\n",
    "    )\n",
    "    # Define loss function\n",
    "    criteria = T.nn.MSELoss()\n",
    "    # pre-allocate memory for loss_fuction\n",
    "    loss_track = np.zeros((params.num_iter, params.num_epochs))\n",
    "\n",
    "    # Training \n",
    "\n",
    "    for p in range(params.num_iter):\n",
    "        for g in range(params.num_epochs):\n",
    "            coeffs.train()\n",
    "\n",
    "            opt_func.zero_grad()\n",
    "\n",
    "            loss_new = T.autograd.Variable(T.tensor([0.0], requires_grad=True))\n",
    "            weights = 2 ** (-0.5 * T.linspace(0, 0, 1))\n",
    "\n",
    "            timesteps_i = T.tensor(np.diff(times, axis=0)).float()\n",
    "            y_total = train_set\n",
    "\n",
    "            # One forward step predictions\n",
    "\n",
    "            y_pred = apply_rk4_SparseId(y_total[:-1], coeffs, times[:-1], timesteps=timesteps_i, Params=params, paramSize=param_size, phases=phases)\n",
    "\n",
    "            # One backward step predictions\n",
    "            y_pred_back = apply_rk4_SparseId(y_total[1:], coeffs, times[1:], timesteps=-timesteps_i, Params=params, paramSize=param_size, phases=phases)\n",
    "\n",
    "            loss_new += criteria(y_pred, y_total[1:]) + weights[0] * criteria(\n",
    "                y_pred_back, y_total[:-1]\n",
    "            )\n",
    "\n",
    "            # loss_new /= y[0].shape[0]\n",
    "            loss_track[p, g] += loss_new.item()\n",
    "            loss_new.backward()\n",
    "            opt_func.step()\n",
    "\n",
    "            sys.stdout.write(\"\\r [Iter %d/%d] [Epoch %d/%d] [Training loss: %.2e] [Learning rate: %.2e]\" % (p + 1, params.num_iter, g + 1, params.num_epochs, loss_track[p, g], opt_func.param_groups[0][\"lr\"],))\n",
    "\n",
    "        # Removing the coefficients smaller than tol and set gradients w.r.t. them to zero\n",
    "        # so that they will not be updated in the iterations\n",
    "        Ws = coeffs.linear.weight.detach().clone()\n",
    "        Mask_Ws = (Ws.abs() > params.tol_coeffs).type(T.float)\n",
    "        coeffs.linear.weight = T.nn.Parameter(Ws * Mask_Ws)\n",
    "\n",
    "        coeffs.linear.weight.register_hook(lambda grad: grad.mul_(Mask_Ws))\n",
    "        new_lr = opt_func.param_groups[0][\"lr\"] / lr_reduction\n",
    "        opt_func = optim_all.RAdam(coeffs.parameters(), lr=new_lr, weight_decay=params.weightdecay)\n",
    "\n",
    "    return coeffs, loss_track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "randSeed = 42\n",
    "\n",
    "T.manual_seed(randSeed)\n",
    "np.random.seed(seed=randSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class parameters:\n",
    "    bs: int = 1\n",
    "    num_epochs: int = 1000\n",
    "    num_iter = 3\n",
    "    lr: float = 1e-2\n",
    "    save_model_path: str = \"./Results/Refactored/\"\n",
    "    weightdecay: float = 0.0\n",
    "    timefinal: float = 50.0\n",
    "    timestep: float = 5e-3\n",
    "    normalize: bool = False\n",
    "    tol_coeffs: float = 5e-2\n",
    "    noise_level: float = 1e-1\n",
    "    noisy_input_flag: bool = False\n",
    "    omega: float = 1.0\n",
    "    zeta: float = 0.05\n",
    "    omega_noise: float = 1e-1\n",
    "    zeta_noise: float = 2e-1\n",
    "    model: str = \"friction\" # \"free\", \"forced\", \"friction\"\n",
    "    forcing_freq: float = 1.2\n",
    "    friction_ratio: float = 0.5\n",
    "    \n",
    "Params = parameters()\n",
    "\n",
    "if Params.noisy_input_flag:\n",
    "    Params.omega = np.random.normal(loc=Params.omega, scale=Params.omega_noise)\n",
    "    Params.zeta = np.random.normal(loc=Params.zeta, scale=Params.zeta_noise)\n",
    "\n",
    "os.makedirs(os.path.dirname(Params.save_model_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate (noisy) measurements - Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.arange(0, Params.timefinal, Params.timestep)\n",
    "\n",
    "# Initial condition and simulation time\n",
    "x0 = [0.1, 0.1]\n",
    "\n",
    "# Solve the equation\n",
    "sol = solve_ivp(lambda t, x: build_true_model(x, t, friction_force_ratio=Params.friction_ratio, forcing_freq=Params.forcing_freq), t_span=[ts[0], ts[-1]], y0=x0, t_eval=ts)\n",
    "\n",
    "x = np.transpose(sol.y)\n",
    "\n",
    "# Generate noisy measurements\n",
    "x = np.random.normal(loc=x, scale=Params.noise_level * np.abs(x), size=x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloaders\n",
    "train_dset = T.tensor(x).float()\n",
    "times = T.tensor(ts).unsqueeze(1).float()\n",
    "\n",
    "phases = np.arange(1., 2.01, 0.1)\n",
    "no_of_terms = apply_features(train_dset[:2], times[:2], phases=phases).shape[1]\n",
    "\n",
    "Coeffs = CoeffsDictionary(no_of_terms)\n",
    "\n",
    "# # Learning Coefficients\n",
    "Coeffs, loss_track = learn_sparse_model(Coeffs, train_dset, times, Params, paramSize=no_of_terms, phases=phases)\n",
    "Learned_Coeffs = Coeffs.linear.weight.detach().clone().t().numpy()\n",
    "\n",
    "Learned_Coeffs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02509e15991341b7a4d4bec687b24d504ce5a8604d44a7eb07adb35c60499661"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
